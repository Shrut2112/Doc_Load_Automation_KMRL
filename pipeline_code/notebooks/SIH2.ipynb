{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSYLNYYwiNhY",
        "outputId": "b2e287b5-010d-466e-93bc-17532c297a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# 1) Install & setup (run once)\n",
        "!pip install -q tqdm regex nltk\n",
        "# If you want lemmatization via spaCy (optional; heavier):\n",
        "# !pip install -q spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from multiprocessing import Pool, cpu_count\n",
        "tqdm.pandas()\n",
        "nltk.download('stopwords')\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVBFg2_HiWAO",
        "outputId": "a6c0b505-349a-4af1-e160-ece599a3fe55"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to input and output inside Google Drive\n",
        "INPUT_CSV = \"/content/drive/MyDrive/Data Folder/all_extracted.csv\"\n",
        "OUTPUT_CSV = \"/content/drive/MyDrive/Data Folder/processed_text.csv\"\n"
      ],
      "metadata": {
        "id": "mL6Y-d3Um8dM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, string, pandas as pd\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "PROCESS_IN_CHUNKS = True   # True = safer for very large files, False = load entire CSV at once\n",
        "CHUNKSIZE = 10000         # used only when PROCESS_IN_CHUNKS = True\n",
        "TEXT_COLUMN = None        # set to your text column name if known (e.g., \"text\"), else auto-detect\n",
        "MIN_TOKEN_LEN = 2\n",
        "\n",
        "# --------------- NLTK: try to enable lemmatization -------------\n",
        "use_nltk = True\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "    # ensure common resources (silent if already present)\n",
        "    for pkg in (\"punkt\", \"stopwords\", \"wordnet\", \"omw-1.4\"):\n",
        "        try:\n",
        "            if pkg == \"punkt\":\n",
        "                nltk.data.find(\"tokenizers/punkt\")\n",
        "            elif pkg == \"omw-1.4\":\n",
        "                nltk.data.find(\"corpora/omw-1.4\")\n",
        "            else:\n",
        "                nltk.data.find(f\"corpora/{pkg}\")\n",
        "        except LookupError:\n",
        "            try:\n",
        "                nltk.download(pkg, quiet=True)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    # load resources (may still fail if offline)\n",
        "    STOPWORDS = set(w.lower() for w in stopwords.words(\"english\"))\n",
        "    TOKENIZER = word_tokenize\n",
        "    LEMMATIZER = WordNetLemmatizer()\n",
        "except Exception:\n",
        "    use_nltk = False\n",
        "    # fallback simple tokenizer & stopwords\n",
        "    def simple_tokenize(s): return s.split()\n",
        "    TOKENIZER = simple_tokenize\n",
        "    STOPWORDS = {\n",
        "        \"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\n",
        "        \"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\n",
        "        \"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\n",
        "        \"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\n",
        "        \"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\n",
        "        \"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\n",
        "        \"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\n",
        "        \"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\n",
        "        \"don\",\"should\",\"now\"\n",
        "    }\n",
        "    try:\n",
        "        from nltk.stem import WordNetLemmatizer\n",
        "        LEMMATIZER = WordNetLemmatizer()\n",
        "    except Exception:\n",
        "        LEMMATIZER = None\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# ------------------ cleaning (keeps your simple style) ----------\n",
        "def clean_text(text):\n",
        "    \"\"\"Replace emails/urls, remove punctuation, lowercase (keeps numbers as is).\"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    s = str(text)\n",
        "    s = re.sub(r'\\S+@\\S+', 'emailaddress', s)\n",
        "    s = re.sub(r'https?://\\S+|www\\.\\S+', 'url', s)\n",
        "    # Remove digits substitution line, keep numbers as is\n",
        "    # s = re.sub(r'\\d+', 'number', s)  <-- REMOVE THIS LINE\n",
        "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Tokenize -> (skip stopwords removal) -> lemmatize if available -> join\"\"\"\n",
        "    txt = clean_text(text)\n",
        "    if not txt:\n",
        "        return \"\"\n",
        "    tokens = TOKENIZER(txt)\n",
        "    tokens = [t.lower() for t in tokens if isinstance(t, str)]\n",
        "    # Skip stopwords removal step altogether (remove the filtering line)\n",
        "    # tokens = [t for t in tokens if len(t) >= MIN_TOKEN_LEN and t not in STOPWORDS]\n",
        "    tokens = [t for t in tokens if len(t) >= MIN_TOKEN_LEN]\n",
        "    # lemmatize if available and nltk resources loaded\n",
        "    if LEMMATIZER is not None and use_nltk:\n",
        "        try:\n",
        "            tokens = [LEMMATIZER.lemmatize(t) for t in tokens]\n",
        "        except Exception:\n",
        "            pass\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "# ----------------- column detection (simple) ---------------------\n",
        "def detect_text_column(df):\n",
        "    # common names first\n",
        "    for c in (\"text\", \"content\", \"message\", \"body\", \"tweet\", \"review\"):\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # otherwise choose object dtype column with highest avg length\n",
        "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
        "    if not obj_cols:\n",
        "        return None\n",
        "    best, best_len = None, -1\n",
        "    for c in obj_cols:\n",
        "        try:\n",
        "            avg = df[c].dropna().astype(str).map(len).mean()\n",
        "            if avg > best_len:\n",
        "                best_len = avg; best = c\n",
        "        except Exception:\n",
        "            continue\n",
        "    return best\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# -------------- processing helpers -------------------------------\n",
        "def process_csv_in_chunks(input_path, output_path, text_col=None, chunksize=CHUNKSIZE):\n",
        "    if not os.path.exists(input_path):\n",
        "        raise FileNotFoundError(f\"Input CSV not found: {input_path}\")\n",
        "    if os.path.exists(output_path):\n",
        "        os.remove(output_path)\n",
        "    reader = pd.read_csv(input_path, chunksize=chunksize, iterator=True, dtype=str, low_memory=False)\n",
        "    first = True\n",
        "    total = 0\n",
        "    for chunk in tqdm(reader, desc=\"Processing (chunks)\"):\n",
        "        total += len(chunk)\n",
        "        if text_col is None:\n",
        "            text_col = detect_text_column(chunk)\n",
        "        if text_col is None:\n",
        "            raise ValueError(\"Could not detect text column. Set TEXT_COLUMN manually.\")\n",
        "        if text_col not in chunk.columns:\n",
        "            raise ValueError(f\"Text column '{text_col}' not found in chunk.\")\n",
        "        chunk[text_col + \"_processed\"] = chunk[text_col].fillna(\"\").astype(str).map(preprocess_text)\n",
        "        if first:\n",
        "            chunk.to_csv(output_path, index=False, mode=\"w\")\n",
        "            first = False\n",
        "        else:\n",
        "            chunk.to_csv(output_path, index=False, header=False, mode=\"a\")\n",
        "    return total, text_col\n",
        "\n",
        "def process_whole_file(input_path, output_path, text_col=None):\n",
        "    if not os.path.exists(input_path):\n",
        "        raise FileNotFoundError(f\"Input CSV not found: {input_path}\")\n",
        "    df = pd.read_csv(input_path, dtype=str, low_memory=False)\n",
        "    if text_col is None:\n",
        "        text_col = detect_text_column(df)\n",
        "    if text_col is None:\n",
        "        raise ValueError(\"Could not detect text column. Set TEXT_COLUMN manually.\")\n",
        "    if text_col not in df.columns:\n",
        "        raise ValueError(f\"Text column '{text_col}' not found in CSV.\")\n",
        "    df[text_col + \"_processed\"] = df[text_col].fillna(\"\").astype(str).map(preprocess_text)\n",
        "    df.to_csv(output_path, index=False)\n",
        "    return len(df), text_col\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# ----------------------- RUN ------------------------------------\n",
        "print(\"INPUT:\", INPUT_CSV)\n",
        "print(\"OUTPUT:\", OUTPUT_CSV)\n",
        "print(\"PROCESS_IN_CHUNKS:\", PROCESS_IN_CHUNKS)\n",
        "try:\n",
        "    if PROCESS_IN_CHUNKS:\n",
        "        total_rows, used_col = process_csv_in_chunks(INPUT_CSV, OUTPUT_CSV, text_col=TEXT_COLUMN, chunksize=CHUNKSIZE)\n",
        "    else:\n",
        "        total_rows, used_col = process_whole_file(INPUT_CSV, OUTPUT_CSV, text_col=TEXT_COLUMN)\n",
        "    print(f\"Done. Rows processed: {total_rows}. Text column used: '{used_col}'.\")\n",
        "    if not use_nltk:\n",
        "        print(\"Note: NLTK not available or resources missing — used fallback tokenization/stopwords. Lemmatization may be skipped.\")\n",
        "    elif LEMMATIZER is None:\n",
        "        print(\"Note: Lemmatizer not available; lemmatization skipped.\")\n",
        "    print(\"Saved processed CSV to:\", OUTPUT_CSV)\n",
        "except Exception as e:\n",
        "    print(\"Processing failed:\", str(e))\n",
        "# ----------------------------------------------------------------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQlTmdJCnboO",
        "outputId": "412dddcc-d580-4a4e-f2f4-4dab9cef43af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT: /content/drive/MyDrive/Data Folder/all_extracted.csv\n",
            "OUTPUT: /content/drive/MyDrive/Data Folder/processed_text.csv\n",
            "PROCESS_IN_CHUNKS: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing (chunks): 2it [00:17,  8.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done. Rows processed: 14235. Text column used: 'text'.\n",
            "Saved processed CSV to: /content/drive/MyDrive/Data Folder/processed_text.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CFyEDm7sn9xj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}